---
title: 有关交叉熵的笔记
date: 2022-07-11 14:58:42
tags: [机器学习, 信息论]
mathjax: true
---

熵这个概念还记得牛哥在一年前给我讲过，计算loss的时候要用到交叉熵，小整理一下。

通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。

要想明白交叉熵(Cross Entropy)的意义，可以从熵(Entropy) -> KL散度(Kullback-Leibler Divergence) -> 交叉熵这个顺序入手。当然，也有多种解释方法[1]。

先给出一个“接地气但不严谨”的概念表述：

- 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。
- KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。
- 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。

一句话总结的话：KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价。

KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度不具备有对称性。在距离上的**对称性**指的是A到B的距离等于B到A的距离。

- 对于离散事件我们可以定义事件A和B的差别为 
  $$
  D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
  $$

- 对于连续事件，那么我们只是把求和改为求积分而已

$$
D_{K L}(A \| B)=\int a(x) \log \left(\frac{a(x)}{b(x)}\right)
$$

事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分：
$$
D_{K L}(A \| B)=-S(A)+H(A, B)
$$
对比一下这是KL散度的公式：
$$
 D_{K L}(A \| B)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(\frac{P_{A}\left(x_{i}\right)}{P_{B}\left(x_{i}\right)}\right)=\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{A}\left(x_{i}\right)\right)-P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
$$
这是熵的公式：
$$
S(A)=-\sum_{i} P_{A}\left(x_{i}\right) \log P_{A}\left(x_{i}\right)
$$
这是交叉熵公式：
$$
H(A, B)=-\sum_{i} P_{A}\left(x_{i}\right) \log \left(P_{B}\left(x_{i}\right)\right)
$$
此处最重要的观察是，如果 $S(A)$ 是一个常量，那么 $D_{K L}(A|| B)=H(A, B)$ ，也就是说 KL散度和交叉熵在特定条件下等价。这个发现是这篇回答的重点。

同时补充交叉熵的一些性质：

- 和KL散度相同，交叉熵也不具备对称性
- 从名字上来看，Cross(交叉)主要是用于描述这是两个事件之间的相互关系，对自己求交叉熵等于熵。即$H(A, A)=S(A)$，注意只是非负而不一定等于0。

一些对比与观察：

- KL散度和交叉熵的不同处：交叉熵中不包括“熵”的部分
- KL散度和交叉熵的相同处：a. 都不具备对称性 b. 都是非负的
- 等价条件：当 $A$ 固定不变时，那么最小化KL散度 $D_{K L}(A \| B)$ 等价于最小化交 叉熵 $H(A, B)$ 。 $D_{K L}(A|| B)=H(A, B)$

最小化模型分布 $P($ model $)$ 与训练数据上的分布 $P($ training $)$ 的差异等 价于最小化这两个分布间的KL散度，也就是最小化 $KL(P(training ) || P( model )) $ 。

巧的是，训练数据的分布 $\mathrm{A}$ 是给定的，那么 求$D_{K L}(A \| B)$ 等价于求 $H(A, B)$ ，也就是A与B的交叉熵。得证，交叉嫡可以用于计算 “学习模型的分布"与 “训练数据分布" 之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。

但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个[高斯分布](https://www.zhihu.com/search?q=高斯分布&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A244557337})的误差，是模型的泛化误差下限。
